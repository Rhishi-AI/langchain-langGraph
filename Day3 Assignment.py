# -*- coding: utf-8 -*-
"""Day3 Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1llf_VTO0oOLwYJFgEc1yW0C_GoCxlAP3
"""

!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --no-cache-dir -q

# For installing the libraries & downloading models from HF Hub
!pip install huggingface_hub==0.35.3 pandas==2.2.2 tiktoken==0.12.0 pymupdf==1.26.5 langchain==0.3.27 langchain-community==0.3.31 chromadb==1.1.1 sentence-transformers==5.1.1 numpy==2.3.3 -q

#Libraries for Loading Data, Chunking, Embedding, and Vector Databases
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

#Libraries for downloading and loading the llm
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
model_basename = "mistral-7b-instruct-v0.2.Q6_K.gguf"
model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename
)

llm = Llama(
    model_path=model_path,
    n_ctx=2048, # Context window
    n_gpu_layers=-1, # Uncomment to use GPU
    n_batch=512, # Batch size for prompt processing
    verbose=True, # Changed to True to bypass fileno error
    f16_kv=True # Key/value cache in float16
)
print("Llama model initialized.")

def Prompt(query,max_tokens=512,temperature=0,top_p=0.5,top_k=20):
    model_output = llm(
      prompt=query,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      top_k=top_k
    )

    return model_output['choices'][0]['text']

# Defining a funcation to validate the different creteria

def score_clarity(prompt: str) -> int:
    """
    Evaluates the clarity of a prompt using an LLM.
    """
    clarity_prompt = f"""Evaluate the clarity of the following prompt on a scale of 0 to 10. A score of 10 means the prompt is perfectly clear and easy to understand. A score of 0 means it's completely unclear. Only respond with the integer score.

Prompt: {prompt}
Clarity Score:"""

    response = Prompt(clarity_prompt)
    try:
        score = int(response.strip().split('/')[0].strip())
        if 0 <= score <= 10:
            return score
    except ValueError:
        pass
    return 5 # Default to a neutral score if parsing fails

def score_specificity(prompt: str) -> int:
    """
    Evaluates the specificity and detail level of a prompt using an LLM.
    """
    specificity_prompt = f"""Evaluate the specificity and detail level of the following prompt on a scale of 0 to 10. A score of 10 means the prompt provides sufficient details and requirements. A score of 0 means it lacks detail. Only respond with the integer score.

Prompt: {prompt}
Specificity Score:"""

    response = Prompt(specificity_prompt)
    try:
        score = int(response.strip().split('/')[0].strip())
        if 0 <= score <= 10:
            return score
    except ValueError:
        pass
    return 5 # Default to a neutral score if parsing fails

def score_context(prompt: str) -> int:
    """
    Evaluates how well the context is provided in a prompt using an LLM.
    """
    context_prompt = f"""Evaluate how well background information, audience, or use case is mentioned in the following prompt on a scale of 0 to 10. A score of 10 means sufficient context is provided. A score of 0 means context is missing. Only respond with the integer score.

Prompt: {prompt}
Context Score:"""

    response = Prompt(context_prompt)
    try:
        score = int(response.strip().split('/')[0].strip())
        if 0 <= score <= 10:
            return score
    except ValueError:
        pass
    return 5 # Default to a neutral score if parsing fails

def score_output_format_constraints(prompt: str) -> int:
    """
    Evaluates if output format and constraints are well-defined in a prompt using an LLM.
    """
    format_constraints_prompt = f"""Evaluate whether the expected output format, tone, or length is specified in the following prompt on a scale of 0 to 10. A score of 10 means format and constraints are clearly defined. A score of 0 means they are not. Only respond with the integer score.

Prompt: {prompt}
Output Format & Constraints Score:"""

    response = Prompt(format_constraints_prompt)
    try:
        score = int(response.strip().split('/')[0].strip())
        if 0 <= score <= 10:
            return score
    except ValueError:
        pass
    return 5 # Default to a neutral score if parsing fails

def score_persona_defined(prompt: str) -> int:
    """
    Evaluates if a persona is clearly defined in a prompt using an LLM.
    """
    persona_prompt = f"""Evaluate whether a specific role is assigned to the AI in the following prompt on a scale of 0 to 10. A score of 10 means a persona is clearly defined. A score of 0 means no persona is defined. Only respond with the integer score.

Prompt: {prompt}
Persona Defined Score:"""

    response = Prompt(persona_prompt)
    try:
        score = int(response.strip().split('/')[0].strip())
        if 0 <= score <= 10:
            return score
    except ValueError:
        pass
    return 5 # Default to a neutral score if parsing fails

print("LLM-based scoring functions updated.")

# Calcualte and explain score

def calculate_and_explain_score(prompt: str):
    """
    Calculates the final prompt quality score and provides an explanation.

    Args:
        prompt (str): The prompt string to evaluate.

    Returns:
        tuple: A tuple containing:
            - float: The final average score.
            - dict: A dictionary of individual scores for each criterion.
            - str: A textual explanation of the prompt's quality.
    """
    scores = {
        "clarity": score_clarity(prompt),
        "specificity": score_specificity(prompt),
        "context": score_context(prompt),
        "output_format_constraints": score_output_format_constraints(prompt),
        "persona_defined": score_persona_defined(prompt)
    }

    final_score = sum(scores.values()) / len(scores)

    explanation_parts = []
    explanation_parts.append(f"The prompt received an overall score of {final_score:.2f} out of 10.")
    explanation_parts.append("Individual scores are:")

    low_score_criteria = []
    for criterion, score in scores.items():
        explanation_parts.append(f"- {criterion.replace('_', ' ').title()}: {score}/10")
        if score < 5: # Arbitrary threshold for 'low' score
            low_score_criteria.append(criterion.replace('_', ' ').title())

    if low_score_criteria:
        explanation_parts.append(f"Consider improving: {', '.join(low_score_criteria)}.")
    else:
        explanation_parts.append("All criteria are met reasonably well.")

    explanation = "\n".join(explanation_parts)

    return final_score, scores, explanation

print("Function `calculate_and_explain_score` defined.")


# Generte a suggestion

def generate_suggestions(scores: dict) -> list[str]:
    """
    Generates improvement suggestions based on individual criterion scores.

    Args:
        scores (dict): A dictionary of individual scores for each criterion.

    Returns:
        list[str]: A list of actionable improvement suggestions.
    """
    suggestions = []
    low_score_threshold = 6  # Criteria scoring below this will trigger suggestions

    if scores.get("clarity", 10) < low_score_threshold:
        suggestions.append(
            "Clarity: Try using simpler language, breaking down complex requests into smaller parts, or providing examples of desired output."
        )
    if scores.get("specificity", 10) < low_score_threshold:
        suggestions.append(
            "Specificity/Details: Add more specific instructions, define key terms, or specify constraints on length, style, or content."
        )
    if scores.get("context", 10) < low_score_threshold:
        suggestions.append(
            "Context: Provide relevant background information, the purpose of the task, or how the output will be used."
        )
    if scores.get("output_format_constraints", 10) < low_score_threshold:
        suggestions.append(
            "Output Format & Constraints: Clearly specify the desired format (e.g., JSON, markdown, bullet points) and any constraints (e.g., word count, tone).")
    if scores.get("persona_defined", 10) < low_score_threshold:
        suggestions.append(
            "Persona defined: Clearly define the role the AI should adopt or the target audience for the generated content."
        )

    if not suggestions:
        suggestions.append("The prompt is well-crafted; no specific improvement suggestions at this time.")

    return suggestions

print("Function `generate_suggestions` defined.")

# Evaluate prompt quality 

def evaluate_prompt_quality(prompt: str):
    """
    Evaluates a prompt based on predefined criteria, generates an explanation, and provides suggestions.

    Args:
        prompt (str): The prompt string to evaluate.

    Returns:
        dict: A dictionary containing:
            - 'final_score': The overall average score.
            - 'individual_scores': A dictionary of scores for each criterion.
            - 'explanation': A textual explanation of the prompt's quality.
            - 'suggestions': A list of improvement suggestions.
    """
    final_score, individual_scores, explanation = calculate_and_explain_score(prompt)
    suggestions = generate_suggestions(individual_scores)

    return {
        "final_score": final_score,
        "individual_scores": individual_scores,
        "explanation": explanation,
        "suggestions": suggestions
    }

print("Function `evaluate_prompt_quality` defined.")

sample_prompts = [
    "Write a short story about a detective solving a mystery in a futuristic city. The story should be around 500 words and have a clear beginning, middle, and end. The detective should be cynical and world-weary. Output in markdown format with headings for introduction, plot, and conclusion.",
    "Summarize the key findings of the latest IPCC report on climate change.",
    "Act as a cheerful tour guide, describing the history of the Eiffel Tower in 200 words. Focus on its construction and initial reception. Output as a bulleted list.",
    "Generate a python function to calculate the factorial of a number.",
    "Compose a haiku about autumn leaves.",
    "Create a JSON object for a product with fields: id, name, price, and category. The product is a 'Laptop', price '1200.00', category 'Electronics', and id 'P001'.",
    "Write a creative advertisement slogan for a new brand of eco-friendly cleaning products."
]

for i, prompt in enumerate(sample_prompts):
    print(f"\n--- Evaluating Prompt {i + 1} ---")
    print(f"Original Prompt: {prompt}")
    evaluation_results = evaluate_prompt_quality(prompt)

    print(f"\nFinal Score: {evaluation_results['final_score']:.2f}/10")
    print("Individual Scores:")
    for criterion, score in evaluation_results['individual_scores'].items():
        print(f"  - {criterion.replace('_', ' ').title()}: {score}/10")

    print("\nExplanation:")
    print(evaluation_results['explanation'])

    print("\nSuggestions:")
    for suggestion in evaluation_results['suggestions']:
        print(f"  - {suggestion}")
    print("-------------------------")
