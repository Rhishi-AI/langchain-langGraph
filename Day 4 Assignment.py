# -*- coding: utf-8 -*-
"""Customer Suport.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WmovKK-GQIh-I1xqCjH7uRhHS-SgrD-3
"""

!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.28 --force-reinstall --no-cache-dir -q

# For installing the libraries & downloading models from HF Hub
!pip install huggingface_hub==0.35.3 pandas==2.2.2 tiktoken==0.12.0 pymupdf==1.26.5 langchain==0.3.27 langchain-community==0.3.31 chromadb==1.1.1 sentence-transformers==5.1.1 numpy==2.3.3 -q

pip install -U langgraph langchain langchain_openai langchain-community

pip install langchain langgraph

#Libraries for processing dataframes,text
import json,os
import tiktoken
import pandas as pd

#Libraries for Loading Data, Chunking, Embedding, and Vector Databases
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma

#Libraries for downloading and loading the llm
from huggingface_hub import hf_hub_download
from llama_cpp import Llama

model_name_or_path = "TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
model_basename = "mistral-7b-instruct-v0.2.Q6_K.gguf"

model_path = hf_hub_download(
    repo_id=model_name_or_path,
    filename=model_basename
)
print(f"The downloaded model is saved at: {model_path}")

llm = Llama(
    model_path=model_path,
    n_ctx=5000,
    n_gpu_layers=38,
    n_batch=512
)

def Prompt(query,max_tokens=512,temperature=0,top_p=0.5,top_k=20):
    model_output = llm(
      prompt=query,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      top_k=top_k
    )

    return model_output['choices'][0]['text']

Prompt("How are you?")

customer_emails = [
    {
        'sender': 'john.doe@example.com',
        'subject': 'Password Reset Request',
        'body': 'I forgot my password and need to reset it. My username is johndoe123. Please provide instructions on how to proceed.'
    },
    {
        'sender': 'jane.smith@example.com',
        'subject': 'Bug Report: Login Issue',
        'body': 'I am unable to log in to my account. After entering my credentials, the page just refreshes without any error message. This started happening today. Browser: Chrome, OS: macOS.'
    },
    {
        'sender': 'urgent.customer@example.com',
        'subject': 'URGENT: Billing Inquiry - Incorrect Charge',
        'body': 'I\'ve noticed an incorrect charge of $49.99 on my last statement dated 2024-05-15. My account number is CUST12345. This needs immediate attention as I did not authorize this payment.'
    },
    {
        'sender': 'feature.fan@example.com',
        'subject': 'Feature Request: Dark Mode Option',
        'body': 'I love your product, but I would really appreciate a dark mode option for the user interface. It would be much easier on the eyes, especially during night use.'
    },
    {
        'sender': 'tech.whiz@example.com',
        'subject': 'Technical Issue: API Integration Failure (Error Code 500)',
        'body': 'I am experiencing an issue with the API integration. When attempting to retrieve data from the /api/v1/data endpoint, I consistently receive a 500 Internal Server Error. I\'ve checked my API key and confirmed it\'s active. The request body is {"param1": "value", "param2": "another_value"}. This occurs on all requests, both from development and production environments. Please advise on troubleshooting steps or a potential fix.'
    }
]

print(f"Generated {len(customer_emails)} simulated customer emails.")

#State

from typing import TypedDict, Literal

# Define the structure for email classification
class EmailClassification(TypedDict):
    intent: Literal["question", "bug", "billing", "feature", "complex"]
    urgency: Literal["low", "medium", "high", "critical"]
    topic: str
    summary: str

class EmailAgentState(TypedDict):
    # Raw email data
    email_content: str
    sender_email: str
    email_id: str

    # Classification result
    classification: EmailClassification | None

    # Raw search/API results
    search_results: list[str] | None  # List of raw document chunks
    customer_history: dict | None  # Raw customer data from CRM

    # Generated content
    draft_response: str | None
    messages: list[str] | None

#Read and classify nodes

from typing import Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import interrupt, Command, RetryPolicy
from langchain_openai import ChatOpenAI
from langchain.messages import HumanMessage
import json # Import json for parsing


def read_email(state: EmailAgentState) -> dict:
    """Extract and parse email content"""
    return {
        "messages": [HumanMessage(content=f"Processing email: {state['email_content']}")]
    }

def classify_intent(state: EmailAgentState) -> Command[Literal["search_documentation"]]: # Updated Literal options
    """Use LLM to classify email intent and urgency, then route accordingly"""

    # Format the prompt to request JSON output matching EmailClassification schema
    classification_prompt = f"""
    Analyze this customer email and classify it.
    Output the classification as a JSON object with the following keys and value types:
    - "intent": one of ["question", "bug", "billing", "feature", "complex"]
    - "urgency": one of ["low", "medium", "high", "critical"]
    - "topic": string, a brief description of the email's main subject
    - "summary": string, a short summary of the email content

    Email: {state['email_content']}
    From: {state['sender_email']}

    JSON Output:
    """

    # Invoke the Llama model using the custom Prompt function
    response_text = Prompt(classification_prompt)

    # Attempt to parse the JSON output from the model
    try:
        json_start = response_text.find('{')
        json_end = response_text.rfind('}')
        if json_start != -1 and json_end != -1 and json_start < json_end:
            json_str = response_text[json_start : json_end + 1]
            classification = json.loads(json_str)
        else:
            raise json.JSONDecodeError("Could not find valid JSON object in response", response_text, 0)
    except json.JSONDecodeError as e:
        print(f"JSON decoding error: {e}")
        print(f"Raw LLM response: {response_text}")
        classification = {
            "intent": "complex",
            "urgency": "medium",
            "topic": "parsing_error",
            "summary": "Could not parse LLM output for classification. Please check model output."
        }

    # Always route to search_documentation now
    return Command(
        update={"classification": classification},
        goto="search_documentation"
    )

def search_and_route(state: EmailAgentState) -> Command[Literal["human_review", "bug_tracking", "draft_response"]]:
    """Simulate search and route based on classification and (dummy) search results."""
    classification = state.get('classification', {}) # Get existing classification
    intent = classification.get('intent', 'unknown')
    urgency = classification.get('urgency', 'medium')
    topic = classification.get('topic', '')

    updated_search_results = []

    # Simulate searching based on intent/topic
    if intent == 'question' or intent == 'feature' or topic == 'Password Reset Request':
        updated_search_results = [
            "Reset password via Settings > Security > Change Password",
            "Password must be at least 12 characters",
            "Include uppercase, lowercase, numbers, and symbols",
            "Dark mode feature is planned for Q4 2024. See release notes for details."
        ]

    # Decision logic for routing
    if urgency == 'critical' or intent in ['billing', 'complex', 'parsing_error', 'unknown']:
        # High-stakes cases always go to human review first
        goto = "human_review"
    elif intent == 'bug':
        # Bugs typically go to bug tracking
        goto = "bug_tracking"
    else:
        # Other intents (question, feature, etc.) go to drafting response
        goto = "draft_response"

    return Command(
        update={
            "classification": classification, # Explicitly pass classification forward
            "search_results": updated_search_results # Update search results
        },
        goto=goto
    )

#Search and tracking nodes

def search_documentation(state: EmailAgentState) -> Command[Literal["human_review", "bug_tracking", "draft_response"]]:
    """Search knowledge base for relevant information and then route based on findings."""

    classification = state.get('classification', {}) # Get existing classification
    query_intent = classification.get('intent', 'unknown')
    query_topic = classification.get('topic', '')
    urgency = classification.get('urgency', 'medium')

    search_results = []
    goto_next_node = "draft_response" # Default route

    try:
        # Simulate your RAG logic here
        # Based on intent/topic, populate dummy search results
        if query_intent == 'question' or query_topic == 'Password Reset Request':
            search_results = [
                "Reset password via Settings > Security > Change Password",
                "Password must be at least 12 characters",
                "Include uppercase, lowercase, numbers, and symbols"
            ]
        elif query_intent == 'feature' and query_topic == 'dark mode':
             search_results = [
                "Dark mode feature is planned for Q4 2024. See release notes for details."
            ]
        elif query_intent == 'bug':
            # For bugs, we might not find direct 'documentation' but the system would proceed to bug tracking
            search_results = [f"No specific documentation found for bug: {query_topic}."]

        # Decide next node based on classification and search results
        if urgency == 'critical' or query_intent in ['billing', 'complex', 'parsing_error', 'unknown']:
            goto_next_node = "human_review"
        elif query_intent == 'bug':
            goto_next_node = "bug_tracking"
        else: # Default for 'question', 'feature', etc.
            goto_next_node = "draft_response"

    except Exception as e: # Catch broader exceptions for this dummy RAG step
        search_results = [f"Search temporarily unavailable or error: {str(e)}"]
        goto_next_node = "human_review" # Escalate on search error

    return Command(
        update={
            "search_results": search_results,  # Store raw results or error
            "classification": classification, # Ensure classification is passed forward
            "next_route_target": goto_next_node # Explicitly store routing decision in state
        },
        goto=goto_next_node # Now explicitly tell LangGraph where to go
    )

def bug_tracking(state: EmailAgentState) -> Command[Literal["draft_response"]]:
    """Create or update bug tracking ticket"""

    # Create ticket in your bug tracking system
    ticket_id = "BUG-12345"  # Would be created via API

    return Command(
        update={
            "search_results": [f"Bug ticket {ticket_id} created"],
            "current_step": "bug_tracked"
        },
        goto="draft_response"
    )

#Response Nodes

def draft_response(state: EmailAgentState) -> Command[Literal["human_review", "send_reply"]]:
    """Generate response using context and route based on quality"""

    classification = state.get('classification', {})
    print(f"[draft_response] Classification received: {classification}") # Debug print

    # Format context from raw state data on-demand
    context_sections = []

    if state.get('search_results'):
        # Format search results for the prompt
        formatted_docs = "\n".join([f"- {doc}" for doc in state['search_results']])
        context_sections.append(f"Relevant documentation:\n{formatted_docs}")

    if state.get('customer_history'):
        # Format customer data for the prompt
        context_sections.append(f"Customer tier: {state['customer_history'].get('tier', 'standard')}")

    # Build the prompt with formatted context
    draft_prompt = f"""
    Draft a response to this customer email:
    {state['email_content']}

    Email intent: {classification.get('intent', 'unknown')}
    Urgency level: {classification.get('urgency', 'medium')}

    {chr(10).join(context_sections)}

    Guidelines:
    - Be professional and helpful
    - Address their specific concern
    - Use the provided documentation when relevant
    """

    # Use the custom Prompt function to get the response text directly
    response_content = Prompt(draft_prompt)

    # Determine if human review needed based on urgency and intent
    urgency_val = classification.get('urgency')
    intent_val = classification.get('intent')
    print(f"[draft_response] Urgency value: {urgency_val}")
    print(f"[draft_response] Intent value: {intent_val}")
    print(f"[draft_response] Urgency in ['high', 'critical']: {urgency_val in ['high', 'critical']}")
    print(f"[draft_response] Intent == 'complex': {intent_val == 'complex'}")

    needs_review = (urgency_val in ['high', 'critical'] or intent_val == 'complex')
    print(f"[draft_response] Needs human review: {needs_review}") # Debug print

    # Route to appropriate next node
    goto = "human_review" if needs_review else "send_reply"
    print(f"[draft_response] Routing to: {goto}") # Debug print

    return Command(
        update={"draft_response": response_content},  # Store the direct text response
        goto=goto
    )

def human_review(state: EmailAgentState) -> Command[Literal["send_reply", END]]:
    """Pause for human review using interrupt and route based on decision"""

    classification = state.get('classification', {})

    # interrupt() must come first - any code before it will re-run on resume
    human_decision = interrupt({
        "email_id": state.get('email_id',''),
        "original_email": state.get('email_content',''),
        "draft_response": state.get('draft_response',''),
        "classification": classification, # Pass the entire classification object
        "action": "Please review and approve/edit this response"
    })

    # Now process the human's decision
    if human_decision.get("approved"):
        return Command(
            update={"draft_response": human_decision.get("edited_response", state.get('draft_response',''))},
            goto="send_reply"
        )
    else:
        # Rejection means human will handle directly
        return Command(update={}, goto=END)

def send_reply(state: EmailAgentState) -> dict:
    """Send the email response"""
    # Integrate with email service
    # Removed: print(f"Sending reply: {state['draft_response'][:100]}...")
    return {}

#Graph compilation code

from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import RetryPolicy

# Create the graph
workflow = StateGraph(EmailAgentState)

# Add nodes with appropriate error handling
workflow.add_node("read_email", read_email)
workflow.add_node("classify_intent", classify_intent)
# Removed workflow.add_node("search_and_route", search_and_route) # Removed this node

# Add retry policy for nodes that might have transient failures
workflow.add_node(
    "search_documentation",
    search_documentation,
    retry_policy=RetryPolicy(max_attempts=3)
)
workflow.add_node("bug_tracking", bug_tracking)
workflow.add_node("draft_response", draft_response)
workflow.add_node("human_review", human_review)
workflow.add_node("send_reply", send_reply)

# Add only the essential edges
workflow.add_edge(START, "read_email")
workflow.add_edge("read_email", "classify_intent")
workflow.add_edge("classify_intent", "search_documentation") # Directly route to search_documentation
# Adjust existing edges
# search_documentation now directly routes to its next node using Command(goto=...)
workflow.add_edge("bug_tracking", "draft_response")
workflow.add_edge("human_review", "send_reply") # Assuming human review leads to send_reply

workflow.add_edge("send_reply", END)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

graph_image = app.get_graph().draw_mermaid_png()

from IPython.display import Image
Image(graph_image)

import io
import contextlib

for idx, email_data in enumerate(customer_emails):
    print(f"\n--- Processing Email {idx + 1} ---")
    individual_initial_state = {
        "email_content": email_data['body'],
        "sender_email": email_data['sender'],
        "email_id": f"email_{idx + 1}",
        "messages": []
    }

    config = {"configurable": {"thread_id": f"customer_email_{idx + 1}"}}

    # Redirect stderr to suppress llama_cpp verbose output during app.invoke
    with contextlib.redirect_stderr(io.StringIO()):
        result = app.invoke(individual_initial_state, config)

    # Display results for the current email
    print("--- Workflow Results ---")
    print(f"Original Email Content: {individual_initial_state['email_content']}")

    # 1) Classified Urgency
    print("\n1) Classified Urgency:")
    # Access classification directly from the result object, which contains the full state
    current_classification = result.get('classification', {})
    print(f"   Urgency: {current_classification.get('urgency', 'N/A')}")

    # 2) Identified Topic
    print("\n2) Identified Topic:")
    print(f"   Topic: {current_classification.get('topic', 'N/A')}")

    # 3) Generated Response Draft:
    print("\n3) Generated Response Draft:")
    if '__interrupt__' in result:
        interrupt_obj = result['__interrupt__'][0]
        draft_content = interrupt_obj.value.get('draft_response', 'N/A - Draft not available at interrupt point or empty.')
        print(f"   Draft (before human edit): {draft_content}")
        print("   Note: This workflow was interrupted for human review. The final edited response is not available directly here without resuming.")
    else:
        print(f"   Final Draft: {result.get('draft_response', 'N/A - Draft not generated.')}")

    # 4) Decision on auto reply vs escalation:
    print("\n4) Decision on auto reply vs escalation:")
    if '__interrupt__' in result:
        print("   Decision: Escalated to human review due to urgency/intent.")
        print("   Action: Workflow paused for human approval.")
    else:
        print("   Decision: Automated reply.")

    # 5) Follow up - actions (e.g., search results, bug tracking info):
    print("\n5) Follow up - actions:")
    if '__interrupt__' in result:
        interrupt_obj = result['__interrupt__'][0]
        search_results_from_interrupt = interrupt_obj.value.get('search_results')
        if search_results_from_interrupt:
            print("   Search Results (at interrupt point):")
            for item in search_results_from_interrupt:
                print(f"     - {item}")
        elif current_classification.get('intent') == 'bug':
            if 'current_step' in interrupt_obj.value and interrupt_obj.value['current_step'] == 'bug_tracked':
                print("   Bug Tracking: A bug ticket was created (before interrupt).")
            else:
                print("   No specific follow-up actions recorded at interrupt point (bug tracking not yet completed).")
        else:
            print("   No specific follow-up actions recorded at interrupt point.")
    elif result.get('search_results'):
        print("   Search Results:")
        for item in result['search_results']:
            print(f"     - {item}")
    elif current_classification.get('intent') == 'bug':
        print("   Bug Tracking: A bug ticket was created.")
    else:
        print("   No specific follow-up actions recorded (other than drafting response).")

    print("\n" + "="*50 + "\n")
